# -*- coding: utf-8 -*-
"""main_clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eAeewYUpEQwJxDqFbPI5LqC5KesT0h9D
"""

# Mount the Google Drive

from google.colab import drive
drive.mount('/content/drive')

# Load the trained model from the Drive

drive_model_path = "/content/drive/MyDrive/models/distilbert_model" #drive path of the saved model

from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast

# Load the model from the google drive
model = DistilBertForSequenceClassification.from_pretrained(drive_model_path)

# Load the tokenizer as well for predictions
tokenizer = DistilBertTokenizerFast.from_pretrained(drive_model_path)

import pandas as pd

# import the latest dataframe with reviews and sentiments
merged_df = pd.read_csv('/content/drive/MyDrive/merged_df.csv')

# See the first few rows
print(merged_df.head())

# print the length of the dataframe
print(len(merged_df))

"""=========================== CLUSTERING ================================"""

# Import required libraries

from google.colab import drive
from sentence_transformers import SentenceTransformer
from sklearn.cluster import MiniBatchKMeans
import numpy as np

# Load the sentence transformer model
model_name = 'all-MiniLM-L6-v2'     # lightweight and efficient
embedding_model = SentenceTransformer(model_name)

# Generate embeddings for the review text
# You can use a different column if `full_review` is not the correct one
print("Generating embeddings...")
embeddings = embedding_model.encode(merged_df['full_review'].tolist(), show_progress_bar=True)

# Determine the optimal number of clusters
# For MiniBatchKMeans, you can try different values for `n_clusters`
n_clusters = 5  # You can adjust this number (4-6) based on analysis

# Perform clustering using MiniBatchKMeans for efficiency
print(f"Clustering into {n_clusters} clusters...")
clustering_model = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, n_init=10)
clustering_model.fit(embeddings)

# Assign cluster labels back to the DataFrame
merged_df['cluster'] = clustering_model.labels_

print("Clustering complete. Inspecting cluster distribution:")
print(merged_df['cluster'].value_counts())

# To inspect the clusters, you can save the DataFrame or sample the results
# merged_df.to_csv('/content/drive/MyDrive/clustered_reviews.csv', index=False)

# Printing Sample Reviews by Cluster

for cluster_id in sorted(merged_df['cluster'].unique()):
    print(f"\n--- Sample Reviews from Cluster {cluster_id} ---")

    # Filter the DataFrame for the current cluster and sample 10 reviews
    cluster_reviews = merged_df[merged_df['cluster'] == cluster_id]

    # Display the first 10 reviews for inspection
    if len(cluster_reviews) > 10:
        sample_reviews = cluster_reviews.sample(10)['full_review']
    else:
        sample_reviews = cluster_reviews['full_review']

    for i, review in enumerate(sample_reviews, 1):
        print(f"{i}. {review[:150]}...") # Print the first 150 characters

# Define the mapping from cluster ID to the new meta-category name
cluster_to_category = {
    0: "General Batteries",
    1: "E-readers & Tablets",
    2: "Branded Batteries",
    3: "Miscellaneous Electronics",
    4: "Smart Home Devices"
}

# Create a new 'meta_category' column in your DataFrame by mapping the cluster IDs
merged_df['meta_category'] = merged_df['cluster'].map(cluster_to_category)

# Display the distribution of the new meta-categories
print("Distribution of Meta-Categories:")
print(merged_df['meta_category'].value_counts())

# Display the DataFrame head with the new column (optional, for verification)
print("\nDataFrame head with new 'meta_category' column:")
print(merged_df.head())

# Define the save path
save_categorised_reviews = '/content/drive/MyDrive/amazon_reviews_categorized.csv'

# Save the DataFrame with the new meta-category column
merged_df.to_csv(save_categorised_reviews, index=False)

# Print a confirmation message using the defined path variable
print(f"DataFrame successfully saved to {save_categorised_reviews}")

# Visualisation of review distribution across Meta-categories

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.countplot(y='meta_category', data=merged_df, order=merged_df['meta_category'].value_counts().index, palette='viridis')
plt.title('Distribution of Reviews Across Meta-Categories')
plt.xlabel('Number of Reviews')
plt.ylabel('Meta-Category')
plt.show()

# Create a cross-tabulation of meta_category and sentiment
sentiment_by_category = pd.crosstab(merged_df['meta_category'], merged_df['sentiment'], normalize='index') * 100

print("\nSentiment Distribution within each Meta-Category (%):")
print(sentiment_by_category)

# Visualize the sentiment distribution per Meta-category
sentiment_by_category.plot(kind='bar', figsize=(12, 7), stacked=True, cmap='coolwarm')
plt.title('Sentiment Distribution per Meta-Category')
plt.xlabel('Meta-Category')
plt.ylabel('Percentage')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Sentiment')
plt.tight_layout()
plt.show()

# Add review_length column if not already present
if 'review_length' not in merged_df.columns:
    merged_df['review_length'] = merged_df['full_review'].apply(len)

plt.figure(figsize=(12, 7))
sns.boxplot(x='meta_category', y='review_length', data=merged_df, palette='muted')
plt.title('Review Length Distribution per Meta-Category')
plt.xlabel('Meta-Category')
plt.ylabel('Review Length')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# save the csv file updated with clusters and meta=categories
save_categoriese_reviews_final = '/content/drive/MyDrive/amazon_reviews_clustered_and_categorized.csv'
merged_df.to_csv(save_categoriese_reviews_final, index=False)

print(f"\nFinal DataFrame with meta-categories saved to: {save_categoriese_reviews_final}")

#Quick Check

# See first few rows with new columns
print(merged_df.head())

# Check only the new columns
print(merged_df[['cluster', 'meta_category']].head())

# Get unique cluster values
print(merged_df['cluster'].unique())

# Get unique meta-categories
print(merged_df['meta_category'].unique())

# See how many items in each cluster
print(merged_df['cluster'].value_counts())

# See how many items in each meta-category
print(merged_df['meta_category'].value_counts())