# -*- coding: utf-8 -*-
"""main_undersampling_till_sentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1806NBm4jGCKQ0ovtkqLgWPJp3zqzAdG1
"""

# ===========================================
# Install MLflow
# ===========================================
!pip install mlflow

# ===========================================
# Step 1: Mount Google Drive
# ===========================================
from google.colab import drive
drive.mount('/content/drive')

# ===========================================
# Step 2: Import Libraries
# ===========================================
import pandas as pd
import glob
import os
import re
import nltk
from nltk.corpus import stopwords
from sklearn.utils import resample
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from datasets import Dataset, DatasetDict
from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding
import mlflow
import mlflow.transformers

# Download stopwords
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# ===========================================
# Step 3: Load and Merge CSV Files
# ===========================================
folder_path = '/content/drive/MyDrive/amazon_reviews/'
csv_files = glob.glob(os.path.join(folder_path, '*.csv'))

df_list = [pd.read_csv(file) for file in csv_files]
all_df = pd.concat(df_list, ignore_index=True)

# Keep only required columns
merged_df = all_df[['reviews.rating', 'reviews.text', 'reviews.title']].copy()
merged_df = merged_df.dropna(subset=['reviews.rating', 'reviews.text'])
merged_df = merged_df.drop_duplicates().reset_index(drop=True)

# ===========================================
# Step 4: Preprocess Text
# ===========================================
def preprocess_text(text):
    text = str(text).lower()
    text = re.sub(r'[^a-z\s]', '', text)
    text = ' '.join([word for word in text.split() if word not in stop_words])
    return text

merged_df['reviews_cleaned'] = merged_df['reviews.text'].apply(preprocess_text)

# ===========================================
# Step 5: Map Sentiment
# ===========================================
def map_sentiment(rating):
    if rating >= 4:
        return 'positive'
    elif rating == 3:
        return 'neutral'
    else:
        return 'negative'

merged_df['sentiment'] = merged_df['reviews.rating'].apply(map_sentiment)
merged_df['full_review'] = merged_df['reviews.title'].fillna('') + ' ' + merged_df['reviews_cleaned']
merged_df = merged_df[['full_review', 'sentiment']]

# ===========================================
# Step 6: Data Visualization
# ===========================================
sns.countplot(x='sentiment', data=merged_df)
plt.title("Sentiment Distribution")
plt.show()

merged_df['review_length'] = merged_df['full_review'].apply(len)
sns.histplot(merged_df['review_length'], bins=50)
plt.title("Review Length Distribution")
plt.show()
merged_df.drop(columns=['review_length'], inplace=True)

# ===========================================
# Step 7a: Oversampling
# ===========================================
# def oversample_data(df):
#     df_positive = df[df['sentiment'] == 'positive']
#     df_neutral = df[df['sentiment'] == 'neutral']
#     df_negative = df[df['sentiment'] == 'negative']

#     max_size = max(len(df_positive), len(df_neutral), len(df_negative))
#     df_positive_over = resample(df_positive, replace=True, n_samples=max_size, random_state=42)
#     df_neutral_over = resample(df_neutral, replace=True, n_samples=max_size, random_state=42)
#     df_negative_over = resample(df_negative, replace=True, n_samples=max_size, random_state=42)

#     balanced_df = pd.concat([df_positive_over, df_neutral_over, df_negative_over])
#     return balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)

# ===========================================
# Step 7b: Undersampling
# ===========================================
def undersample_data(df):
    df_positive = df[df['sentiment'] == 'positive']
    df_neutral = df[df['sentiment'] == 'neutral']
    df_negative = df[df['sentiment'] == 'negative']

    min_size = min(len(df_positive), len(df_neutral), len(df_negative))
    df_positive_under = resample(df_positive, replace=False, n_samples=min_size, random_state=42)
    df_neutral_under = resample(df_neutral, replace=False, n_samples=min_size, random_state=42)
    df_negative_under = resample(df_negative, replace=False, n_samples=min_size, random_state=42)

    balanced_df = pd.concat([df_positive_under, df_neutral_under, df_negative_under])
    return balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)

# Choose method here
merged_df_balanced = undersample_data(merged_df)  # <-- Change to oversample_data(merged_df) if needed
print("Class distribution after balancing:")
print(merged_df_balanced['sentiment'].value_counts())

# Visualization after balancing dataset through undersampling

plt.figure(figsize=(6,4))
merged_df_balanced['sentiment'].value_counts().plot(kind='bar', color=['skyblue', 'lightgreen', 'salmon'])

plt.title("Sentiment Class Distribution After Undersampling")
plt.xlabel("Sentiment")
plt.ylabel("Number of Samples")
plt.xticks(rotation=0)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# ===========================================
# Step 8: Train-Test Split
# ===========================================
train_full_review, test_full_review, train_sentiment, test_sentiment = train_test_split(
    merged_df_balanced['full_review'].tolist(),
    merged_df_balanced['sentiment'].tolist(),
    test_size=0.2,
    random_state=42,
    stratify=merged_df_balanced['sentiment']
)

train_df = Dataset.from_dict({'review': train_full_review, 'sentiment': train_sentiment})
test_df = Dataset.from_dict({'review': test_full_review, 'sentiment': test_sentiment})
dataset = DatasetDict({'train': train_df, 'test': test_df})

# Convert sentiment to integers
sentiment_to_int = {'negative': 0, 'neutral': 1, 'positive': 2}
dataset = dataset.map(lambda x: {'labels': sentiment_to_int[x['sentiment']]})

# ===========================================
# Step 9: Tokenize
# ===========================================
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

def tokenize(batch):
    return tokenizer(batch['review'], padding='max_length', truncation=True, max_length=256)

dataset = dataset.map(tokenize, batched=True)
dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])

# ===========================================
# Step 0: Mount Google Drive (for persistence)
# ===========================================
from google.colab import drive
drive.mount('/content/drive')

# Set a path to store models permanently in Drive
drive_model_path = "/content/drive/MyDrive/models/distilbert_model"

# ===========================================
# Step 10: Load Model
# ===========================================
from transformers import DistilBertForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import mlflow
import mlflow.transformers

model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)

# ===========================================
# Step 11: Training with MLflow
# ===========================================
mlflow.set_tracking_uri("file:///content/drive/MyDrive/mlruns")  # Store MLflow logs/artifacts in Drive
mlflow.set_experiment("amazon_reviews_sentiment")

with mlflow.start_run(run_name="distilbert_balanced"):
    # Log parameters
    mlflow.log_param("model_name", "distilbert-base-uncased")
    mlflow.log_param("num_train_epochs", 3)
    mlflow.log_param("train_batch_size", 16)
    mlflow.log_param("eval_batch_size", 32)

    # Training arguments
    training_args = TrainingArguments(
        output_dir='./results',
        num_train_epochs=3,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=32,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir='./logs',
        logging_steps=100,
        eval_strategy='epoch',
        save_strategy='epoch',
        load_best_model_at_end=True,
        metric_for_best_model='accuracy',
        report_to='none'
    )

    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    def compute_metrics(pred):
        labels = pred.label_ids
        preds = pred.predictions.argmax(-1)
        return {
            'accuracy': accuracy_score(labels, preds),
            'precision': precision_score(labels, preds, average='weighted'),
            'recall': recall_score(labels, preds, average='weighted'),
            'f1': f1_score(labels, preds, average='weighted')
        }

    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset['train'],
        eval_dataset=dataset['test'],
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
        data_collator=data_collator
    )

    # Train
    trainer.train()

    # Evaluate
    preds_output = trainer.predict(dataset['test'])
    metrics = compute_metrics(preds_output)
    print("Evaluation metrics:", metrics)

    for key, value in metrics.items():
        mlflow.log_metric(key, value)

    # ===========================================
    # Save model locally and in Google Drive
    # ===========================================
    local_model_path = "./distilbert_model"
    trainer.save_model(local_model_path)

    # Copy model to Google Drive for safe storage
    !cp -r ./distilbert_model "$drive_model_path"

    # ===========================================
    # Log model to MLflow (with artifact path)
    # ===========================================
    mlflow.transformers.log_model(
        transformers_model=local_model_path,
        artifact_path="distilbert_model",
        task="text-classification"
    )

print(f"Model saved to {drive_model_path} and logged in MLflow.")

# ===========================================
# Step 12: Evaluation and Confusion Matrix
# ===========================================
labels = preds_output.label_ids
preds = preds_output.predictions.argmax(-1)

print("\nClassification Report:\n")
print(classification_report(labels, preds, target_names=['negative', 'neutral', 'positive']))

cm = confusion_matrix(labels, preds)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['negative','neutral','positive'],
            yticklabels=['negative','neutral','positive'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# ===========================================
# Step 13: Display random reviews from each class
# ===========================================

def display_random_reviews(df, sentiment, n=5):
    """Displays n random reviews for a given sentiment."""
    print(f"\n--- Random {sentiment.capitalize()} Reviews ---")
    reviews = df[df['sentiment'] == sentiment]['full_review'].sample(n, random_state=42).tolist()
    for i, review in enumerate(reviews):
        print(f"{i+1}. {review}\n")

# Display 5 random reviews from each sentiment class
display_random_reviews(merged_df_balanced, 'negative')
display_random_reviews(merged_df_balanced, 'neutral')
display_random_reviews(merged_df_balanced, 'positive')

# ==============================
# Step 14: Testing the model
# ==============================

import torch

def predict_sentiment(text):
    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)
    # Move input tensors to the same device as the model
    inputs = {name: tensor.to(model.device) for name, tensor in inputs.items()}
    outputs = model(**inputs)
    pred = outputs.logits.argmax(-1).item()
    return ['negative','neutral','positive'][pred]

# Example:
print(predict_sentiment("Waste of money...Short lifespan awful far ive used toothbrush replace times months")) #negative
print(predict_sentiment("good like first going slow sometimes overall good product")) #neutral
print(predict_sentiment("love love alexa smart house items works great alexa makes life lil stress free")) #positive

!ls -l "./distilbert_model"

!mkdir -p "/content/drive/MyDrive/models" #creates a directory

!cp -r ./distilbert_model "/content/drive/MyDrive/models/" #creates a copy of the model in the directory

# ==============================
# Step 15: Save the DataFrame
# ==============================

import pandas as pd
from google.colab import drive

# Define the path where you want to save the DataFrame
df_save_path = '/content/drive/MyDrive/merged_df.csv'

# Save the DataFrame to the specified path
# The `index=False` argument prevents pandas from saving the DataFrame's index as a column
merged_df.to_csv(df_save_path, index=False)